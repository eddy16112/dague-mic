extern "C" %{
  /**
   * PLASMA include for defined and constants.
   *
   * @precisions normal z -> s d c
   *
   */
#include <plasma.h>
#include <core_blas.h>

#include "dague.h"
#include "data_distribution.h"
#include "memory_pool.h"
#include "dplasma/lib/dplasmajdf.h"

#include "data_dist/sparse-matrix/pastix_internal/pastix_internal.h"
#include "data_dist/sparse-matrix/sparse-matrix.h"
#include "dsparse/lib/core_z.h"

#define PRECISION_z

#if defined(HAVE_CUDA) && defined(PRECISION_s)
#include "gpu_data.h"
#include "dsparse/cores/cuda_sparse_sgemm.h"
extern int *gpu_counter;
#endif  /* defined(HAVE_CUDA) && defined(PRECISION_s) */
%}

/* Globals
 */
descA    [type = "sparse_matrix_desc_t *"]
A        [type = "dague_ddesc_t *"]
datacode [type = "SolverMatrix*" default="&(descA->pastix_data->solvmatr)" hidden=on]
cblknbr  [type = "dague_int_t"   default="descA->pastix_data->solvmatr.symbmtx.cblknbr - 1" hidden=on] 
bloknbr  [type = "dague_int_t"   default="descA->pastix_data->solvmatr.symbmtx.bloknbr - 2" hidden=on] /* -2 to avoid last diagonal block */
p_work   [type = "dague_memory_pool_t *" size = "datacode->coefmax * sizeof(Dague_Complex64_t)"]

/**************************************************
 *                POTRF_TRSM                      *
 **************************************************/
POTRF_TRSM(k) [high_priority = on]

// Execution space
k = 0..cblknbr
gcblk2list= inline_c %{ return UPDOWN_GCBLK2LIST(UPDOWN_LOC2GLOB( k )); %}
browk     = inline_c %{ if ( gcblk2list != -1 ) return UPDOWN_LISTPTR( gcblk2list   ); else return -1; %}
browk1    = inline_c %{ if ( gcblk2list != -1 ) return UPDOWN_LISTPTR( gcblk2list+1 ); else return -1; %}
lastbrow  = inline_c %{ if (browk1 > 0) return UPDOWN_LISTBLOK(browk1-1); else return 0; %}
firstblok = inline_c %{ return SYMB_BLOKNUM(k); %}
lastblok  = inline_c %{ return SYMB_BLOKNUM(k+1); %}
// Parallel partitioning
:A(k) // Should match SOLV_COEFTAB(k)

// Parameters
/* C is A(k) if it's a leaf or get the cblk from the last update */
RW A <- ( browk == browk1 ) ? A(k) : C GEMM( lastbrow )
     -> A GEMM(firstblok+1..lastblok-1)
     -> A(k)

BODY
#if defined(HAVE_CUDA) && defined(PRECISION_s)
      sparse_gpu_mark_data_usage( (sparse_matrix_desc_t*)__dague_object->super.A, DAGUE_READ | DAGUE_WRITE, k );
#endif  /* defined(HAVE_CUDA) && defined(PRECISION_s) */

      DRYRUN(
             core_zpotrfsp1d(A, datacode, k, descA->pastix_data->sopar.espilondiag );
	     );
      printlog(
               "thread %d compute_1dplus( cblknum=%d, browk=%d, browk1=%d, lastbrow=%d, firstblok=%d, lastblok=%d )\n",
               context->eu_id, k, browk, browk1, lastbrow, firstblok, lastblok);

END

/**************************************************
 *                      GEMM                      *
 **************************************************/

GEMM(k)

// Execution space
k = 0..bloknbr
fcblk  = inline_c %{ return SYMB_CBLKNUM(k); %}
cblk   = inline_c %{ return sparse_matrix_get_lcblknum( descA, (dague_int_t)k ); %}// TODO: store cblknum in symbblok
phony  = inline_c %{ return k == SYMB_BLOKNUM( cblk ); %}
prev   = inline_c %{ if (phony) return 0; else return sparse_matrix_get_listptr_prev( descA, (dague_int_t)k, (dague_int_t)fcblk ); %}
next   = inline_c %{ if (phony) return 0; else return sparse_matrix_get_listptr_next( descA, (dague_int_t)k, (dague_int_t)fcblk ); %}

// Parallel partitioning
:A(fcblk)

// Parameters
READ  A <- phony ? A(fcblk) : A POTRF_TRSM(cblk)
RW    C <- ( prev == 0 ) ? A(fcblk) : C GEMM( prev )
        -> phony ? A(fcblk)
        -> ((!phony) && (next == 0)) ? A POTRF_TRSM( fcblk )
        -> ((!phony) && (next != 0)) ? C GEMM( next )

BODY
      if (!phony) {
#if defined(HAVE_CUDA) && defined(PRECISION_s)
          sparse_gpu_mark_data_usage( (sparse_matrix_desc_t*)__dague_object->super.A, DAGUE_READ, cblk );
          if( dague_using_gpu() > 0 ) {
              int rc;
              
              if( 0 == (rc = sparse_gpu_sgemm( context, this_task, PlasmaUpper )) ) {
                  printlog(
                           "thread %d compute_1dgemm( k=%d, fcblk=%d, cblk=%d, prev=%d, next=%d ) - GPU(0)\n",
                           context->eu_id, k, fcblk, cblk, prev, next);
                  goto FIN;
              }
              if( -1 == rc ) {
                  printlog(
                           "thread %d compute_1dgemm( k=%d, fcblk=%d, cblk=%d, prev=%d, next=%d ) - GPU(-1)\n",
                           context->eu_id, k, fcblk, cblk, prev, next);
                  /* We're done, but the task has been already destroyed */
                  return -1;
              }
              if( -2 == rc ) {
                  /* The GPU failed to execute this task, but the task was already rescheduled */
                  fprintf(stderr, "Unable to disable GPU at runtime. Fatal error.\n");
                  exit(2);
              }
              /* Continue with the task on the cores */
          }
          sparse_gpu_mark_data_usage( (sparse_matrix_desc_t*)__dague_object->super.A, DAGUE_READ | DAGUE_WRITE, fcblk );
#endif  /* defined(HAVE_CUDA) && defined(PRECISION_s) */

          Dague_Complex64_t *work = (Dague_Complex64_t *)dague_private_memory_pop( p_work );

          DRYRUN(
                 core_zpotrfsp1d_gemm(cblk, k, fcblk, A, C, work, datacode);
                 );
          printlog(
                   "thread %d compute_1dgemm( k=%d, fcblk=%d, cblk=%d, prev=%d, next=%d )\n",
                   context->eu_id, k, fcblk, cblk, prev, next);

          dague_private_memory_push( p_work, (void *)work );
      } else {
          printlog(
                   "thread %d phony_gemm( k=%d, fcblk=%d, cblk=%d, prev=%d, next=%d )\n",
                   context->eu_id, k, fcblk, cblk, prev, next);
      }

#if defined(HAVE_CUDA) && defined(PRECISION_s)
FIN:
#endif
END
