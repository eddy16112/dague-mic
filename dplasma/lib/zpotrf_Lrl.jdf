extern "C" %{
/*
 *  Copyright (c) 2010
 *
 *  The University of Tennessee and The University
 *  of Tennessee Research Foundation.  All rights
 *  reserved.
 *
 * @precisions normal z -> s d c
 *
 */
#define PRECISION_z

#include <plasma.h>
#include <core_blas.h>

#include "dague.h"
#include "data_distribution.h"
#include "data_dist/matrix/precision.h"
#include "data_dist/matrix/matrix.h"
#include "dplasma/lib/memory_pool.h"
#include "dplasma/lib/dplasmajdf.h"
#include "data_dist/matrix/two_dim_rectangle_cyclic.h"
#include "data_dist/matrix/sym_two_dim_rectangle_cyclic.h"
#include "scheduling.h"

#include "zgemm_wrapper.c"
#include "zherk_wrapper.c"
#include "ztrsm_wrapper.c"


#if defined(HAVE_CUDA) && defined(PRECISION_s)
#include "gpu_data.h"
#include "dplasma/cores/cuda_zgemm.h"
extern int *gpu_counter;
#endif  /* defined(HAVE_CUDA) && defined(PRECISION_s) */

//#define MYGEMM
//#define MYHERK
//#define MYTRSM
//#define MYPOTRF

#define KERNEL_POTRF	1
#define KERNEL_TRSM		2
#define KERNEL_HERK		3
#define KERNEL_GEMM		4

#define SMALL_TILE_RATE 5
#define SMALL_TILE_THRESHOLD	192

typedef struct {
    dague_execution_unit_t *context;
    dague_execution_context_t *task;
    dague_ddesc_t * A;
    dague_ddesc_t * B;
    dague_ddesc_t * C;
    int cb_kernel_type;
} callback_data;

static int complete_recursive_dague_callback(dague_object_t* dague_object, void* cb_data)
{
    int rc = 0;
    callback_data* casted_data = (callback_data*)cb_data;

    rc = dague_complete_execution(casted_data->context, casted_data->task);

    if (casted_data->cb_kernel_type == KERNEL_GEMM) {
        free(casted_data->A);
        free(casted_data->B);
        free(casted_data->C);
        dplasma_zgemm_Destruct(dague_object);
    } else if (casted_data->cb_kernel_type == KERNEL_HERK) {
        free(casted_data->A);
        free(casted_data->B);
        dplasma_zherk_Destruct(dague_object);
    } else if (casted_data->cb_kernel_type == KERNEL_TRSM) {
        free(casted_data->A);
        free(casted_data->B);
        dplasma_ztrsm_Destruct(dague_object);
    } else if (casted_data->cb_kernel_type == KERNEL_POTRF) {
        free(casted_data->A);
        dplasma_zpotrf_Destruct(dague_object);
    }

    free(casted_data);
    return rc;
}

%}


/* Globals
 */
PRI_CHANGE [type = int]
uplo       [type = PLASMA_enum]
descA      [type = "tiled_matrix_desc_t"]
A          [type = "dague_ddesc_t *"]
INFO       [type = "int*"]

/**************************************************
 *                      POTRF                     *
 **************************************************/
POTRF(k) [high_priority = on]

// Execution space
k = 0..descA.mt-1

// Parallel partitioning
:A(k, k)

// Parameters
RW T <- (k == 0) ? A(k, k) : T HERK(k-1, k)
     -> T TRSM(k+1..descA.mt-1, k)
     -> A(k, k)

; inline_c %{
    if (descA.mb == SMALL_TILE_THRESHOLD) {
//        printf("small\n");
        return (k >= (descA.mt - PRI_CHANGE)) ? (descA.mt - k) * (descA.mt - k) * (descA.mt - k) : 0x7fffffff;
    } else {
        return (k >= (descA.mt - PRI_CHANGE)) ? (descA.mt - k) * (descA.mt - k) * (descA.mt - k) : 1000000000;
    } 
%}
//; (k >= (descA.mt - PRI_CHANGE)) ? (descA.mt - k + 7) * (descA.mt - k + 2) * (descA.mt - k) / 6 : 0

BODY

    int tempkm = k == descA.mt-1 ? descA.m - k*descA.mb : descA.mb;
    int ldak = BLKLDD( descA, k );

#if defined(HAVE_CUDA) && defined(PRECISION_s)
    if (tempkm > 310) {
    gpu_mark_data_usage( (tiled_matrix_desc_t*)__dague_object->super.A, DAGUE_READ | DAGUE_WRITE, k, k );
    }
#endif  /* defined(HAVE_CUDA) && defined(PRECISION_s) */

#ifdef MYPOTRF
    if (tempkm > SMALL_TILE_THRESHOLD) {

        two_dim_block_cyclic_t *small_descT;
        small_descT = (two_dim_block_cyclic_t *) malloc(sizeof(two_dim_block_cyclic_t));

        int small_mb = descA.mb / SMALL_TILE_RATE;
        int small_nb = descA.nb / SMALL_TILE_RATE;

        int iinfo = 0;
        two_dim_block_cyclic_init(small_descT, matrix_ComplexDouble, matrix_Lapack,
                                  1, 1, 0, small_mb, small_nb, ldak, tempkm,
                                  0, 0, tempkm, tempkm, 1, 1, 1);
        small_descT->mat = T;

        dague_object_t* dague_zpotrf = dplasma_zpotrf_New(uplo, (tiled_matrix_desc_t *)small_descT, &iinfo );
        callback_data *potrf_cb_data = (callback_data *) malloc(sizeof(callback_data));
        potrf_cb_data->context = context;
        potrf_cb_data->task = this_task;
        potrf_cb_data->A = (dague_ddesc_t *) small_descT;
        potrf_cb_data->cb_kernel_type = KERNEL_POTRF;
        dague_set_complete_callback(dague_zpotrf, complete_recursive_dague_callback, (void *)potrf_cb_data);
        dague_enqueue(context->master_context, dague_zpotrf);
 //       printf("POTRF DAGUE\n");
		return -1;

    } else {
        DRYRUN(
            int iinfo = 0;
            CORE_zpotrf(
                uplo, tempkm, T, ldak,
                &iinfo );
            if ( iinfo != 0 && *INFO == 0 )
                *INFO = k*descA.mb+iinfo; /* Should return here */
               );
    }
#else
    DRYRUN(
        int iinfo = 0;
        CORE_zpotrf(
            uplo, tempkm, T, ldak,
            &iinfo );
        if ( iinfo != 0 && *INFO == 0 )
            *INFO = k*descA.mb+iinfo; /* Should return here */
           );
#endif

    printlog(
        "thread %d CORE_zpotrf( %d ) {%d}\n\t( %s, %d, A(%d,%d)[%p], %d)\n",
        context->eu_id, k, this_task->priority,
        plasma_const(uplo), tempkm, k, k, T, descA.mb );

END


/**************************************************
 *                      TRSM                      *
 **************************************************/
TRSM(m, k) [high_priority = on]

// Execution space
m = 1..descA.mt-1
k = 0..m-1

// Parallel partitioning
: A(m, k)

// Parameters
READ  T <- T POTRF(k)
RW    C <- (k == 0) ? A(m, k) : C GEMM(k-1, m, k)
        -> A HERK(k, m)
        -> A GEMM(k, m, k+1..m-1 )
        -> B GEMM(k, m+1..descA.mt-1, m )
        -> A(m, k)

; inline_c %{
    if (descA.mb == SMALL_TILE_THRESHOLD) {
        return (m >= (descA.mt - PRI_CHANGE)) ? (descA.mt - m) * (descA.mt - m) * (descA.mt - m) + 3 * ((2 * descA.mt) - k - m - 1) * (m - k) : 0x7fffffff-1; 
    } else {
        return (m >= (descA.mt - PRI_CHANGE)) ? (descA.mt - m) * (descA.mt - m) * (descA.mt - m) + 3 * ((2 * descA.mt) - k - m - 1) * (m - k) : 1000000000;
    } 
%}
//;  (m >= (descA.mt - PRI_CHANGE)) ? (descA.mt - m - k) * ((descA.mt - m - k + 1) / 2 + 2) - 1 + (descA.mt - m + 2) * (descA.mt - m + 1) * (descA.mt - m) / 6 : 0

BODY

    int tempmm = m == descA.mt-1 ? descA.m - m * descA.mb : descA.mb;
    int ldak = BLKLDD( descA, k );
    int ldam = BLKLDD( descA, m );
#if defined(HAVE_CUDA) && defined(PRECISION_s)
    if (tempmm > 310 || descA.nb > 310) {
    gpu_mark_data_usage( (tiled_matrix_desc_t*)__dague_object->super.A, DAGUE_READ, k, k );
    gpu_mark_data_usage( (tiled_matrix_desc_t*)__dague_object->super.A, DAGUE_READ | DAGUE_WRITE, m, k );
    }
#endif  /* defined(HAVE_CUDA) && defined(PRECISION_s) */

    //int tempmm = m == descA.mt-1 ? descA.m - m * descA.mb : descA.mb;
    //int ldak = BLKLDD( descA, k );
    //int ldam = BLKLDD( descA, m );

#ifdef MYTRSM
    if (tempmm > SMALL_TILE_THRESHOLD || descA.nb > SMALL_TILE_THRESHOLD) {
        two_dim_block_cyclic_t *small_descT, *small_descC;
        small_descT = (two_dim_block_cyclic_t *) malloc(sizeof(two_dim_block_cyclic_t));
        small_descC = (two_dim_block_cyclic_t *) malloc(sizeof(two_dim_block_cyclic_t));

        int small_mb = descA.mb / SMALL_TILE_RATE;
        int small_nb = descA.nb / SMALL_TILE_RATE;

        two_dim_block_cyclic_init(small_descT, matrix_ComplexDouble, matrix_Lapack,
                                  1, 1, 0, small_mb, small_nb, ldak, descA.nb,
                                  0, 0, descA.nb, descA.nb, 1, 1, 1);
        two_dim_block_cyclic_init(small_descC, matrix_ComplexDouble, matrix_Lapack,
                                  1, 1, 0, small_mb, small_nb, ldam, descA.nb,
                                  0, 0, tempmm, descA.nb, 1, 1, 1);

        small_descT->mat = T;
        small_descC->mat = C;

        dague_object_t* dague_ztrsm = dplasma_ztrsm_New(PlasmaRight,  PlasmaLower,
                                                        PlasmaConjTrans, PlasmaNonUnit,
                                                        (Dague_Complex64_t)1.0,
                                                        (tiled_matrix_desc_t *)small_descT,
                                                        (tiled_matrix_desc_t *)small_descC );

        callback_data *trsm_cb_data = (callback_data *) malloc(sizeof(callback_data));
        trsm_cb_data->context = context;
        trsm_cb_data->task = this_task;
        trsm_cb_data->A = (dague_ddesc_t *) small_descT;
        trsm_cb_data->B = (dague_ddesc_t *) small_descC;
        trsm_cb_data->cb_kernel_type = KERNEL_TRSM;
        dague_set_complete_callback(dague_ztrsm, complete_recursive_dague_callback, (void *)trsm_cb_data);
        dague_enqueue(context->master_context, dague_ztrsm);
        return -1;

    } else {
        DRYRUN(
            CORE_ztrsm(
                PlasmaRight, PlasmaLower, PlasmaConjTrans, PlasmaNonUnit,
                tempmm, descA.nb,
                (Dague_Complex64_t)1.0, T /*A(k, k)*/, ldak,
                C /*A(m, k)*/, ldam);
               );
    }
#else
    DRYRUN(
        CORE_ztrsm(
            PlasmaRight, PlasmaLower, PlasmaConjTrans, PlasmaNonUnit,
            tempmm, descA.nb,
            (Dague_Complex64_t)1.0, T /*A(k, k)*/, ldak,
                                    C /*A(m, k)*/, ldam);
           );
#endif

    printlog("thread %d CORE_ztrsm( %d, %d )\t{%d}\n\t( %s, %s, %s, %s, %d, %d, %f, A(%d,%d)[%p], %d,  A(%d,%d)[%p], %d)\n",
             context->eu_id, m, k, this_task->priority,
             plasma_const( PlasmaRight ), plasma_const( PlasmaLower ),
             plasma_const( PlasmaConjTrans ), plasma_const( PlasmaNonUnit ),
             tempmm, descA.nb,
             1.0, k, k, T, ldak,
                  m, k, C, ldam);

END


/**************************************************
 *                      HERK                      *
 **************************************************/
HERK(k, m) [high_priority = on]

// Execution space
k = 0..descA.mt-2
m = k+1..descA.mt-1

// Parallel partitioning
: A(m, m)

//Parameters
READ  A <- C TRSM(m, k)
RW    T <- (k == 0)   ? A(m, m)    : T HERK(k-1, m)
        -> (m == k+1) ? T POTRF(m) : T HERK(k+1, m)


; inline_c %{
    if (descA.mb == SMALL_TILE_THRESHOLD) {
        return (m >= (descA.mt - PRI_CHANGE)) ? (descA.mt - m) * (descA.mt - m) * (descA.mt - m) + 3 * (m - k) : 0x7fffffff-1;
    } else {
        return (m >= (descA.mt - PRI_CHANGE)) ? (descA.mt - m) * (descA.mt - m) * (descA.mt - m) + 3 * (m - k) : 1000000000;
    }
%}

//; (m >= (descA.mt - PRI_CHANGE)) ? (descA.mt - m) * (descA.mt - m) * (descA.mt - m) + 3 * (m - k) : 1000000000
//; (m >= (descA.mt - PRI_CHANGE)) ? (descA.mt - m + 2) * (descA.mt - m + 1) * (descA.mt - m) / 6 + descA.mt - m + k + 1 : 0

BODY

    int tempmm = m == descA.mt-1 ? descA.m - m*descA.mb : descA.mb;
    int ldam = BLKLDD( descA, m );

#if defined(HAVE_CUDA) && defined(PRECISION_s)
//    printf("m: %d, k: %d\n", m, k);
	if (tempmm > 310 || descA.mb > 310) {
    gpu_mark_data_usage( (tiled_matrix_desc_t*)__dague_object->super.A, DAGUE_READ, m, k );
    gpu_mark_data_usage( (tiled_matrix_desc_t*)__dague_object->super.A, DAGUE_READ | DAGUE_WRITE, m, m );
    }
#endif  /* defined(HAVE_CUDA) && defined(PRECISION_s) */

    //int tempmm = m == descA.mt-1 ? descA.m - m*descA.mb : descA.mb;
    //int ldam = BLKLDD( descA, m );

#ifdef MYHERK
    if (tempmm > SMALL_TILE_THRESHOLD || descA.mb > SMALL_TILE_THRESHOLD) {
        int small_mb = descA.mb / SMALL_TILE_RATE;
        int small_nb = descA.nb / SMALL_TILE_RATE;

        two_dim_block_cyclic_t *small_descA, *small_descT;
        small_descA = (two_dim_block_cyclic_t *) malloc(sizeof(two_dim_block_cyclic_t));
        small_descT = (two_dim_block_cyclic_t *) malloc(sizeof(two_dim_block_cyclic_t));

        two_dim_block_cyclic_init(small_descA, matrix_ComplexDouble, matrix_Lapack,
                                  1, 1, 0, small_mb, small_nb, ldam, descA.mb,
                                  0, 0, tempmm, descA.mb, 1, 1, 1);
        two_dim_block_cyclic_init(small_descT, matrix_ComplexDouble, matrix_Lapack,
                                  1, 1, 0, small_mb, small_nb, ldam, descA.mb,
                                  0, 0, tempmm, tempmm, 1, 1, 1);
        small_descA->mat = A;
        small_descT->mat = T;
//        printf("MY HERK\n");
        dague_object_t *dague_zherk = dplasma_zherk_New( PlasmaLower, PlasmaNoTrans, (double)-1.0,
                                                         (tiled_matrix_desc_t*) small_descA, (double)1.0,
                                                         (tiled_matrix_desc_t*) small_descT);

        callback_data *herk_cb_data = (callback_data *) malloc(sizeof(callback_data));
        herk_cb_data->context = context;
        herk_cb_data->task = this_task;
        herk_cb_data->A = (dague_ddesc_t *) small_descA;
        herk_cb_data->B = (dague_ddesc_t *) small_descT;
        herk_cb_data->cb_kernel_type = KERNEL_HERK;
        dague_set_complete_callback(dague_zherk, complete_recursive_dague_callback, (void *)herk_cb_data);
        dague_enqueue(context->master_context, dague_zherk);
        return -1;

    } else {
        DRYRUN(
            CORE_zherk(
                PlasmaLower, PlasmaNoTrans,
                tempmm, descA.mb,
                (double)-1.0, A /*A(m, k)*/, ldam,
                (double) 1.0, T /*A(m, m)*/, ldam);
               );
    }
#else
    DRYRUN(
        CORE_zherk(
            PlasmaLower, PlasmaNoTrans,
            tempmm, descA.mb,
            (double)-1.0, A /*A(m, k)*/, ldam,
            (double) 1.0, T /*A(m, m)*/, ldam);
        );
#endif

    printlog(
             "thread %d CORE_zherk( %d, %d ) {%d}\n\t( %s, %s, %d, %d, %f, A(%d,%d)[%p], %d, %f, A(%d,%d)[%p], %d)\n",
             context->eu_id, k, m, this_task->priority,
             plasma_const( PlasmaLower ), plasma_const( PlasmaNoTrans ),
             tempmm, descA.mb,
             -1.0, m, k, A, ldam,
              1.0, m, m, T, ldam);
END

/**************************************************
 *                      GEMM                      *
 **************************************************/
// Name
GEMM(k, m, n)

// Execution space
k = 0..descA.mt-3
m = k+2..descA.mt-1
n = k+1..m-1

// Parallel partitioning
: A(m, n)

// Parameters
READ  A <- C TRSM(m, k)
READ  B <- C TRSM(n, k)
RW    C <- (k == 0)   ? A(m, n)      : C GEMM(k-1, m, n)
        -> (n == k+1) ? C TRSM(m, n) : C GEMM(k+1, m, n)

; inline_c %{
    if (descA.mb == SMALL_TILE_THRESHOLD) {
        return (m >= (descA.mt - PRI_CHANGE)) ? (descA.mt - m) * (descA.mt - m) * (descA.mt - m) + 3 * ((2 * descA.mt) - m - n - 3) * (m - n) + 6 * (m - k) : 0x7fffffff-2;
    } else {
        return (m >= (descA.mt - PRI_CHANGE)) ? (descA.mt - m) * (descA.mt - m) * (descA.mt - m) + 3 * ((2 * descA.mt) - m - n - 3) * (m - n) + 6 * (m - k) : 1000000000;
    }
%}

//; (m >= (descA.mt - PRI_CHANGE)) ? (descA.mt - m) * (descA.mt - m) * (descA.mt - m) + 3 * ((2 * descA.mt) - m - n - 3) * (m - n) + 6 * (m - k) : 1000000000
//;  (m >= (descA.mt - PRI_CHANGE)) ? (descA.mt - m - n) * ((descA.mt - n - k + 1) / 2 + 2) - 1 + (descA.mt - m + 2) * (descA.mt - m + 1) * (descA.mt - m) / 6 + descA.mt - n + k + 1 : 0

BODY
    int tempmm = m == descA.mt-1 ? descA.m - m * descA.mb : descA.mb;
    int ldam = BLKLDD( descA, m );
    int ldan = BLKLDD( descA, n );

#ifdef HAVE_CUDA
	//printf("HAVE_CUDA OK\n");
#endif

#ifdef PRECISION_s
	//printf("PRECISION_s OK\n");
#endif

#if defined(HAVE_CUDA) && defined(PRECISION_s)
	if (tempmm > 310 || descA.mb > 310) {
    gpu_mark_data_usage( (tiled_matrix_desc_t*)__dague_object->super.A, DAGUE_READ, m, k );
    gpu_mark_data_usage( (tiled_matrix_desc_t*)__dague_object->super.A, DAGUE_READ, n, k );
    //printf("gpu_mark_data_usage OK\n");
	//printf("dague_using_gpu :%d\n", dague_using_gpu());
    if( zgemm_cuda_ndevices() > 0 ) {
        int rc;
        rc = gpu_zgemm( context, this_task, uplo );
        if( 0 == rc) {
			printf("rc 0\n");
            goto FIN;
		}
        if( -1 == rc ) {
            /* We're done, but the task has been already destroyed */
            return -1;
        }
        if( -2 == rc ) {
            /* The GPU failed to execute this task, but the task was already rescheduled */
            fprintf(stderr, "Unable to disable GPU at runtime. Fatal error.\n");
            exit(2);
        }
		//fprintf(stderr, "#####continuing with the task on the cores\n");
        /* Continue with the task on the cores */
    }
    gpu_mark_data_usage( (tiled_matrix_desc_t*)__dague_object->super.A, DAGUE_READ | DAGUE_WRITE, m, n );
	}
	else {
		goto MYCPU;
	}
#endif  /* defined(HAVE_CUDA) && defined(PRECISION_s) */

#if defined(HAVE_CUDA) && defined(PRECISION_s)
MYCPU:
#endif

#ifdef MYGEMM
    if (tempmm > SMALL_TILE_THRESHOLD || descA.mb > SMALL_TILE_THRESHOLD) {
        int small_mb = descA.mb / SMALL_TILE_RATE;
        int small_nb = descA.nb / SMALL_TILE_RATE;
  //      printf("gemm dague\n");
        two_dim_block_cyclic_t *small_descA, *small_descB, *small_descC;
        small_descA = (two_dim_block_cyclic_t *) malloc(sizeof(two_dim_block_cyclic_t));
        small_descB = (two_dim_block_cyclic_t *) malloc(sizeof(two_dim_block_cyclic_t));
        small_descC = (two_dim_block_cyclic_t *) malloc(sizeof(two_dim_block_cyclic_t));

        two_dim_block_cyclic_init(small_descA, matrix_ComplexDouble, matrix_Lapack,
                                  1, 1, 0, small_mb, small_nb, ldam, descA.nb,
                                  0, 0, tempmm, descA.nb, 1, 1, 1);
        two_dim_block_cyclic_init(small_descB, matrix_ComplexDouble, matrix_Lapack,
                                  1, 1, 0, small_mb, small_nb, ldan, descA.nb,
                                  0, 0, descA.mb, descA.nb, 1, 1, 1);
        two_dim_block_cyclic_init(small_descC, matrix_ComplexDouble, matrix_Lapack,
                                  1, 1, 0, small_mb, small_nb, ldam, descA.nb,
                                  0, 0, tempmm, descA.nb, 1, 1, 1);
        small_descA->mat = A;
        small_descB->mat = B;
        small_descC->mat = C;

        dague_object_t *dague_zgemm = dplasma_zgemm_New(PlasmaNoTrans, PlasmaConjTrans, (Dague_Complex64_t)-1.0,
                                                        (tiled_matrix_desc_t *)small_descA, (tiled_matrix_desc_t *)small_descB,
                                                        (Dague_Complex64_t) 1.0,  (tiled_matrix_desc_t *)small_descC);

        callback_data *gemm_cb_data = (callback_data *) malloc(sizeof(callback_data));
        gemm_cb_data->context = context;
        gemm_cb_data->task = this_task;
        gemm_cb_data->A = (dague_ddesc_t *) small_descA;
        gemm_cb_data->B = (dague_ddesc_t *) small_descB;
        gemm_cb_data->C = (dague_ddesc_t *) small_descC;
        gemm_cb_data->cb_kernel_type = KERNEL_GEMM;
        dague_set_complete_callback(dague_zgemm, complete_recursive_dague_callback, (void *)gemm_cb_data);
        dague_enqueue(context->master_context, dague_zgemm);
        return -1;

    } else {
        DRYRUN(
            CORE_zgemm(
                PlasmaNoTrans, PlasmaConjTrans,
                tempmm, descA.mb, descA.mb,
                (Dague_Complex64_t)-1.0, A /*A(m, k)*/, ldam,
                B /*A(n, k)*/, ldan,
                (Dague_Complex64_t) 1.0, C /*A(m, n)*/, ldam);
               );
    }
#else
    DRYRUN(
        CORE_zgemm(
            PlasmaNoTrans, PlasmaConjTrans,
            tempmm, descA.mb, descA.mb,
            (Dague_Complex64_t)-1.0, A /*A(m, k)*/, ldam,
                                     B /*A(n, k)*/, ldan,
            (Dague_Complex64_t) 1.0, C /*A(m, n)*/, ldam);
        );
#endif

    printlog("thread %d CORE_zgemm( %d, %d, %d ) {%d}\n\t( %s, %s, %d, %d, %d, %f, A(%d,%d)[%p], %d, A(%d,%d)[%p], %d, %f, A(%d,%d)[%p], %d)\n",
             context->eu_id, k, m, n, this_task->priority,
             plasma_const( PlasmaNoTrans ),  plasma_const( PlasmaConjTrans ),
             tempmm, descA.mb, descA.mb,
             -1.0, m, k, A, ldam,
                   n, k, B, ldan,
             1.0,  m, n, C, ldam);

#if defined(HAVE_CUDA) && defined(PRECISION_s)
FIN:
#endif
END
