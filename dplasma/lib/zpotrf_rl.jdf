extern "C" %{
/*
 *  Copyright (c) 2010      
 * 
 *  The University of Tennessee and The University
 *  of Tennessee Research Foundation.  All rights
 *  reserved.
 *
 * @precisions normal z -> s d c
 *
 */
#define PRECISION_z

#include <plasma.h>
#include <core_blas.h>

#include "dague.h"
#include "data_distribution.h"
#include "data_dist/matrix/precision.h"
#include "dplasma/lib/dplasmajdf.h"


#if defined(HAVE_CUDA) && defined(PRECISION_s)
#include "gpu_data.h"
#include "cores/cuda_sgemm.h"
extern int *gpu_counter;
#endif
%}

/* Globals
 */
A            [type = "struct tiled_matrix_desc_t*"]
NB           [type = int]
SIZE         [type = int]
PRI_CHANGE   [type = int]
uplo         [type = PLASMA_enum]
INFO         [type = "int*"]

/**************************************************
 *                      POTRF                     *
 **************************************************/
POTRF(k) [high_priority = on]

// Execution space
k = 0..SIZE-1

// Parallel partitioning
: A(k, k)

// Parameters
RW T <- (k == 0) ? A(k, k) : T HERK(k-1, k)
     -> T TRSM(k, k+1..SIZE-1)
     -> A(k, k)

; (k >= (SIZE - PRI_CHANGE)) ? (SIZE - k) * (SIZE - k) * (SIZE - k) : 1000000000
//; (k >= (SIZE - PRI_CHANGE)) ? (SIZE - k + 7) * (SIZE - k + 2) * (SIZE - k) / 6 : 0

BODY
#if defined(HAVE_CUDA) && defined(PRECISION_s)
    gpu_mark_data_usage( (tiled_matrix_desc_t*)__dague_object->super.A, DAGUE_READ | DAGUE_WRITE, k, k );
#endif  /* defined(HAVE_CUDA) && defined(PRECISION_s) */
    DRYRUN(
        CORE_zpotrf(
            uplo,
            NB, /*k == A.nt-1 ? A.n-k*A.nb : A.nb,*/
            T /* A(k, k) */, NB, /*A.nb,*/
            INFO )
        );
    assert( 0 == (*INFO) );
    printlog(
             "thread %d CORE_potrf( %s, %d, A(%d,%d), %d)\n",
             context->eu_id, (uplo == PlasmaLower) ? "PlasmaLower" : "PlasmaUpper",
             NB, /*k == A.nt-1 ? A.n-k*A.nb : A.nb,*/
             k, k, NB /*A.nb,*/ );
END


/**************************************************
 *                      TRSM                      *
 **************************************************/
TRSM(k, n) [high_priority = on]

// Execution space
k = 0..SIZE-1
n = k+1..SIZE-1

// Parallel partitioning
: A(n, k)

// Parameters
READ  T <- T POTRF(k)
RW    C <- (k == 0) ? A(n, k) : C GEMM(k-1, n, k)
        -> A HERK(k, n)
        -> A GEMM(k, n+1..SIZE-1, n)
        -> B GEMM(k, n, k+1..n-1)
        -> A(n, k)

; (n >= (SIZE - PRI_CHANGE)) ? (SIZE - n) * (SIZE - n) * (SIZE - n) + 3 * (2 * SIZE - k - n - 1) * (n - k) : 1000000000
//;  (n >= (SIZE - PRI_CHANGE)) ? (SIZE - n - k) * ((SIZE - n - k + 1) / 2 + 2) - 1 + (SIZE - n + 2) * (SIZE - n + 1) * (SIZE - n) / 6 : 0

BODY
#if defined(HAVE_CUDA) && defined(PRECISION_s)
    gpu_mark_data_usage( (tiled_matrix_desc_t*)__dague_object->super.A, DAGUE_READ, k, k );
    gpu_mark_data_usage( (tiled_matrix_desc_t*)__dague_object->super.A, DAGUE_READ | DAGUE_WRITE, n, k );
#endif  /* defined(HAVE_CUDA) && defined(PRECISION_s) */

    if( uplo == PlasmaLower ) {
        DRYRUN(
            CORE_ztrsm(
                PlasmaRight, PlasmaLower, PlasmaTrans, PlasmaNonUnit,
                NB, /*m == A.nt-1 ? A.n-m*A.nb : A.nb,*/
                NB, /*A.nb,*/
                (Dague_Complex64_t)1.0, T /*A(k, k)*/, NB, /*A.nb,*/
                                        C /*A(n, k)*/, NB /*A.nb*/ )
            );
        printlog( 
                 "thread %d CORE_trsm( %s, %s, %s, %s, %d, %d, %f, A(%d,%d), %d, A(%d,%d), %d)\n",
                 context->eu_id, "PlasmaRight", "PlasmaLower", "PlasmaTrans", "PlasmaNonUnit",
                 NB, /*m == A.nt-1 ? A.n-m*A.nb : A.nb,*/
                 NB, /*A.nb,*/
                 1.0, k, k, NB, /*A.nb,*/
                 n, k, NB /*A.nb*/ );

    } else {
        DRYRUN(
            CORE_ztrsm(
                PlasmaLeft, PlasmaUpper, PlasmaTrans, PlasmaNonUnit,
                NB, /*m == A.nt-1 ? A.n-m*A.nb : A.nb,*/
                NB, /*A.nb,*/
                (Dague_Complex64_t)1.0, T /*A(k, k)*/, NB, /*A.nb,*/
                                        C /*A(k, n)*/, NB /*A.nb*/ )
            );
       printlog(
                "thread %d CORE_trsm( %s, %s, %s, %s, %d, %d, %f, A(%d,%d), %d, A(%d,%d), %d)\n",
                context->eu_id, "PlasmaLeft", "PlasmaUpperr", "PlasmaTrans", "PlasmaNonUnit",
                NB, /*m == A.nt-1 ? A.n-m*A.nb : A.nb,*/
                NB, /*A.nb,*/
                1.0, k, k, NB, /*A.nb,*/
                k, n, NB /*A.nb*/ );
    }
END


/**************************************************
 *                      HERK                      *
 **************************************************/
HERK(k, n) [high_priority = on]

// Execution space
k = 0..SIZE-1
n = k+1..SIZE-1

// Parallel partitioning
: A(n, n)

//Parameters
READ  A <- C TRSM(k, n)
RW    T <- (k == 0) ? A(n,n) : T HERK(k-1, n)
        -> (n == k+1) ? T POTRF(k+1) : T HERK(k+1,n)

; (n >= (SIZE - PRI_CHANGE)) ? (SIZE - n) * (SIZE - n) * (SIZE - n) + 3 * (n - k) : 1000000000
//; (n >= (SIZE - PRI_CHANGE)) ? (SIZE - n + 2) * (SIZE - n + 1) * (SIZE - n) / 6 + SIZE - n + k + 1 : 0

BODY
#if defined(HAVE_CUDA) && defined(PRECISION_s)
    gpu_mark_data_usage( (tiled_matrix_desc_t*)__dague_object->super.A, DAGUE_READ, n, k );
    gpu_mark_data_usage( (tiled_matrix_desc_t*)__dague_object->super.A, DAGUE_READ | DAGUE_WRITE, n, n );
#endif  /* defined(HAVE_CUDA) && defined(PRECISION_s) */

    if( uplo == PlasmaLower ) {
        DRYRUN(
            CORE_zherk(
                PlasmaLower, PlasmaNoTrans,
                NB, /*k == A.nt-1 ? A.n-k*A.nb : A.nb,*/
                NB, /*A.nb,*/
                (double)-1.0, A /*A(n, k)*/, NB, /*A.nb,*/
                (double) 1.0, T /*A(n, n)*/, NB /*A.nb*/ )
            );
        printlog(
                 "thread %d CORE_herk( %s, %s, %d, %d, %f, A(%d,%d), %d, %f, A(%d,%d), %d)\n",
                 context->eu_id, "PlasmaLower", "PlasmaNoTrans",
                 NB, /*k == A.nt-1 ? A.n-k*A.nb : A.nb,*/
                 NB, /*A.nb,*/
                 -1.0, n, k, NB, /*A.nb,*/
                 1.0, n, n, NB /*A.nb*/ );
    } else {
        DRYRUN(
            CORE_zherk(
                PlasmaUpper, PlasmaTrans,
                NB, /*k == A.nt-1 ? A.n-k*A.nb : A.nb,*/
                NB, /*A.nb,*/
                (double)-1.0, A /*A(k, n)*/, NB, /*A.nb,*/
                (double) 1.0, T /*A(n, n)*/, NB /*A.nb*/ )
            );
        printlog(
                 "thread %d CORE_herk( %s, %s, %d, %d, %f, A(%d,%d), %d, %f, A(%d,%d), %d)\n",
                 context->eu_id, "PlasmaLower", "PlasmaNoTrans",
                 NB, /*k == A.nt-1 ? A.n-k*A.nb : A.nb,*/
                 NB, /*A.nb,*/
                 -1.0, k, n, NB, /*A.nb,*/
                 1.0, n, n, NB /*A.nb*/ );
    }
END

/**************************************************
 *                      GEMM                      *
 **************************************************/
// Name
GEMM(k, m, n)

// Execution space
k = 0..SIZE-1
m = k+2..SIZE-1
n = k+1..m-1

// Parallel partitioning
: A(m, n)

// Parameters
READ  A <- C TRSM(k, n)
READ  B <- C TRSM(k, m)
RW    C <- (k == 0) ? A(m, n) : C GEMM(k-1, m, n)
        -> (n == k+1) ? C TRSM(k+1, m) : C GEMM(k+1, m, n)

; (m >= (SIZE - PRI_CHANGE)) ? (SIZE - m) * (SIZE - m) * (SIZE - m) + 3 * (2 * SIZE - m - n - 3) * (m - n) + 6 * (m - k) : 1000000000
//;  (m >= (SIZE - PRI_CHANGE)) ? (SIZE - m - n) * ((SIZE - n - k + 1) / 2 + 2) - 1 + (SIZE - m + 2) * (SIZE - m + 1) * (SIZE - m) / 6 + SIZE - n + k + 1 : 0

BODY
#if defined(HAVE_CUDA) && defined(PRECISION_s)
    gpu_mark_data_usage( (tiled_matrix_desc_t*)__dague_object->super.A, DAGUE_READ, n, k );
    gpu_mark_data_usage( (tiled_matrix_desc_t*)__dague_object->super.A, DAGUE_READ, m, k );
    if( dague_using_gpu() > 0 ) {
        int rc;

        if( 0 == (rc = gpu_sgemm( context, this_task, uplo )) )
            goto FIN;
        if( -1 == rc ) {
            /* We're done, but the task has been already destroyed */
            return -1;
        }
        if( -2 == rc ) {
            /* The GPU failed to execute this task, but the task was already rescheduled */
	    fprintf(stderr, "Unable to disable GPU at runtime. Fatal error.\n");
	    exit(2);
        }
        /* Continue with the task on the cores */
    }
    gpu_mark_data_usage( (tiled_matrix_desc_t*)__dague_object->super.A, DAGUE_READ | DAGUE_WRITE, m, n );
#endif  /* defined(HAVE_CUDA) && defined(PRECISION_s) */

    if( uplo == PlasmaLower ) {
        DRYRUN(
            CORE_zgemm( 
                PlasmaNoTrans, PlasmaTrans,
                NB, /*m == A.nt-1 ? A.n-m*A.nb : A.nb,*/
                NB, /*A.nb,*/
                NB, /*A.nb,*/
                (Dague_Complex64_t)-1.0, B /*A(m, k)*/, NB, /*A.nb,*/
                                         A /*A(n, k)*/, NB, /*A.nb,*/
                (Dague_Complex64_t) 1.0, C /*A(m, n)*/, NB /*A.nb*/ )
            );
        printlog(
                 "thread %d CORE_gemm( %s, %s, %d, %d, %d, %f, A(%d,%d), %d, A(%d,%d), %d, %f, A(%d,%d), %d)\n",
                 context->eu_id, "PlasmaNoTrans", "PlasmaTrans",
                 NB, /*m == A.nt-1 ? A.n-m*A.nb : A.nb,*/
                 NB, /*A.nb,*/
                 NB, /*A.nb,*/
                 -1.0, n, k, NB, /*A.nb,*/
                       m, k, NB, /*A.nb,*/
                  1.0, m, n, NB /*A.nb*/ );
    } else {
        DRYRUN(
            CORE_zgemm(
                PlasmaTrans, PlasmaNoTrans,
                NB, /*m == A.nt-1 ? A.n-m*A.nb : A.nb,*/
                NB, /*A.nb,*/
                NB, /*A.nb,*/
                (Dague_Complex64_t)-1.0, A /*A(k, n)*/, NB, /*A.nb,*/
                                         B /*A(k, m)*/, NB, /*A.nb,*/
                (Dague_Complex64_t) 1.0, C /*A(n, m)*/, NB /*A.nb*/ )
            );
        printlog(
                 "%d CORE_gemm( %s, %s, %d, %d, %d, %f, A(%d,%d), %d, A(%d,%d), %d, %f, A(%d,%d), %d)\n",
                 context->eu_id, "PlasmaNoTrans", "PlasmaTrans",
                 NB, /*m == A.nt-1 ? A.n-m*A.nb : A.nb,*/
                 NB, /*A.nb,*/
                 NB, /*A.nb,*/
                 -1.0, k, n, NB, /*A.nb,*/
                       k, m, NB, /*A.nb,*/
                 1.0, n, m, NB /*A.nb*/ );
    }
#if defined(HAVE_CUDA) && defined(PRECISION_s)
FIN:
#endif
END
